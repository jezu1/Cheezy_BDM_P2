{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5d8591d7-075a-4dc6-8d1a-9d855d0e2dd7",
   "metadata": {},
   "source": [
    "#### Importing Libraries and creating the connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dae0535d-ac36-4838-b3b4-869c40e837e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install neo4jupyter\n",
    "#!pip install py2neo\n",
    "#pip install delta-spark\n",
    "#pip install neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cad66dbb-df8d-4d0d-947b-434440b3eb1b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyarrow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdelta\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtables\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msql\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctions\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m----> 5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpyarrow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfs\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mfs\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdelta\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mre\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyarrow'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyspark\n",
    "from delta.tables import *\n",
    "from pyspark.sql.functions import *\n",
    "import pyarrow.fs as fs\n",
    "from delta import *\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from neo4j import GraphDatabase\n",
    "from py2neo import Graph\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "714699d2-5d82-4c3d-9e07-0dca648caaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the logging level to a higher level to reduce verbosity\n",
    "logging.getLogger().setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "817008d0-86a6-4e70-93bd-4b9783deb501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the log level to WARN\n",
    "os.environ['SPARK_LOG_LEVEL'] = 'WARN'\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--conf spark.driver.memory=2g --conf spark.executor.memory=2g pyspark-shell'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fedda23-3f3d-4461-aa0a-3e750b9663af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set log level to ERROR for Spark-related logs\n",
    "# logger = SparkSession.builder.getOrCreate()._jvm.org.apache.log4j\n",
    "# logger.LogManager.getLogger(\"org\").setLevel(logger.Level.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1facc321-14dc-49fc-ac4b-02e303024cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/hadoop/anaconda3/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/hadoop/.ivy2/cache\n",
      "The jars for the packages stored in: /home/hadoop/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-9ec4e10f-31d1-409a-8f67-94ad4c7e45e1;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.1.0 in central\n",
      "\tfound io.delta#delta-storage;2.1.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in central\n",
      "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in central\n",
      ":: resolution report :: resolve 414ms :: artifacts dl 14ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.12;2.1.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.1.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-9ec4e10f-31d1-409a-8f67-94ad4c7e45e1\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 4 already retrieved (0kB/22ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/08 18:41:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/06/08 18:41:45 WARN DependencyUtils: Local jar /neo4j-connector/neo4j-connector-apache-spark_2.12_3.0-4.0.0.jar does not exist, skipping.\n",
      "23/06/08 18:41:45 INFO SparkContext: Running Spark version 3.3.0\n",
      "23/06/08 18:41:46 INFO ResourceUtils: ==============================================================\n",
      "23/06/08 18:41:46 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "23/06/08 18:41:46 INFO ResourceUtils: ==============================================================\n",
      "23/06/08 18:41:46 INFO SparkContext: Submitted application: DeltaLakeToNeo4j\n",
      "23/06/08 18:41:46 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "23/06/08 18:41:46 INFO ResourceProfile: Limiting resource is cpu\n",
      "23/06/08 18:41:46 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "23/06/08 18:41:46 INFO SecurityManager: Changing view acls to: hadoop\n",
      "23/06/08 18:41:46 INFO SecurityManager: Changing modify acls to: hadoop\n",
      "23/06/08 18:41:46 INFO SecurityManager: Changing view acls groups to: \n",
      "23/06/08 18:41:46 INFO SecurityManager: Changing modify acls groups to: \n",
      "23/06/08 18:41:46 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(hadoop); groups with view permissions: Set(); users  with modify permissions: Set(hadoop); groups with modify permissions: Set()\n",
      "23/06/08 18:41:46 INFO Utils: Successfully started service 'sparkDriver' on port 46631.\n",
      "23/06/08 18:41:46 INFO SparkEnv: Registering MapOutputTracker\n",
      "23/06/08 18:41:46 INFO SparkEnv: Registering BlockManagerMaster\n",
      "23/06/08 18:41:46 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "23/06/08 18:41:46 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "23/06/08 18:41:46 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "23/06/08 18:41:46 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-accb6808-5a53-4f29-b11c-260b0d799b48\n",
      "23/06/08 18:41:46 INFO MemoryStore: MemoryStore started with capacity 1007.8 MiB\n",
      "23/06/08 18:41:47 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "23/06/08 18:41:47 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "23/06/08 18:41:47 ERROR SparkContext: Failed to add /neo4j-connector/neo4j-connector-apache-spark_2.12_3.0-4.0.0.jar to Spark environment\n",
      "java.io.FileNotFoundException: Jar /neo4j-connector/neo4j-connector-apache-spark_2.12_3.0-4.0.0.jar not found\n",
      "\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:1949)\n",
      "\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2004)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$12(SparkContext.scala:507)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$12$adapted(SparkContext.scala:507)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:507)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "23/06/08 18:41:47 INFO SparkContext: Added file file:///home/hadoop/.ivy2/jars/io.delta_delta-core_2.12-2.1.0.jar at file:///home/hadoop/.ivy2/jars/io.delta_delta-core_2.12-2.1.0.jar with timestamp 1686249705953\n",
      "23/06/08 18:41:47 INFO Utils: Copying /home/hadoop/.ivy2/jars/io.delta_delta-core_2.12-2.1.0.jar to /tmp/spark-2a7d7f5d-e051-45d0-86ba-f2db8e507a91/userFiles-976854ae-e41a-4c1e-b4d4-09ce6e839fc3/io.delta_delta-core_2.12-2.1.0.jar\n",
      "23/06/08 18:41:47 INFO SparkContext: Added file file:///home/hadoop/.ivy2/jars/io.delta_delta-storage-2.1.0.jar at file:///home/hadoop/.ivy2/jars/io.delta_delta-storage-2.1.0.jar with timestamp 1686249705953\n",
      "23/06/08 18:41:47 INFO Utils: Copying /home/hadoop/.ivy2/jars/io.delta_delta-storage-2.1.0.jar to /tmp/spark-2a7d7f5d-e051-45d0-86ba-f2db8e507a91/userFiles-976854ae-e41a-4c1e-b4d4-09ce6e839fc3/io.delta_delta-storage-2.1.0.jar\n",
      "23/06/08 18:41:47 INFO SparkContext: Added file file:///home/hadoop/.ivy2/jars/org.antlr_antlr4-runtime-4.8.jar at file:///home/hadoop/.ivy2/jars/org.antlr_antlr4-runtime-4.8.jar with timestamp 1686249705953\n",
      "23/06/08 18:41:47 INFO Utils: Copying /home/hadoop/.ivy2/jars/org.antlr_antlr4-runtime-4.8.jar to /tmp/spark-2a7d7f5d-e051-45d0-86ba-f2db8e507a91/userFiles-976854ae-e41a-4c1e-b4d4-09ce6e839fc3/org.antlr_antlr4-runtime-4.8.jar\n",
      "23/06/08 18:41:47 INFO SparkContext: Added file file:///home/hadoop/.ivy2/jars/org.codehaus.jackson_jackson-core-asl-1.9.13.jar at file:///home/hadoop/.ivy2/jars/org.codehaus.jackson_jackson-core-asl-1.9.13.jar with timestamp 1686249705953\n",
      "23/06/08 18:41:47 INFO Utils: Copying /home/hadoop/.ivy2/jars/org.codehaus.jackson_jackson-core-asl-1.9.13.jar to /tmp/spark-2a7d7f5d-e051-45d0-86ba-f2db8e507a91/userFiles-976854ae-e41a-4c1e-b4d4-09ce6e839fc3/org.codehaus.jackson_jackson-core-asl-1.9.13.jar\n",
      "23/06/08 18:41:47 INFO Executor: Starting executor ID driver on host heracross.fib.upc.es\n",
      "23/06/08 18:41:47 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
      "23/06/08 18:41:47 INFO Executor: Fetching file:///home/hadoop/.ivy2/jars/io.delta_delta-storage-2.1.0.jar with timestamp 1686249705953\n",
      "23/06/08 18:41:47 INFO Utils: /home/hadoop/.ivy2/jars/io.delta_delta-storage-2.1.0.jar has been previously copied to /tmp/spark-2a7d7f5d-e051-45d0-86ba-f2db8e507a91/userFiles-976854ae-e41a-4c1e-b4d4-09ce6e839fc3/io.delta_delta-storage-2.1.0.jar\n",
      "23/06/08 18:41:47 INFO Executor: Fetching file:///home/hadoop/.ivy2/jars/org.antlr_antlr4-runtime-4.8.jar with timestamp 1686249705953\n",
      "23/06/08 18:41:47 INFO Utils: /home/hadoop/.ivy2/jars/org.antlr_antlr4-runtime-4.8.jar has been previously copied to /tmp/spark-2a7d7f5d-e051-45d0-86ba-f2db8e507a91/userFiles-976854ae-e41a-4c1e-b4d4-09ce6e839fc3/org.antlr_antlr4-runtime-4.8.jar\n",
      "23/06/08 18:41:47 INFO Executor: Fetching file:///home/hadoop/.ivy2/jars/io.delta_delta-core_2.12-2.1.0.jar with timestamp 1686249705953\n",
      "23/06/08 18:41:47 INFO Utils: /home/hadoop/.ivy2/jars/io.delta_delta-core_2.12-2.1.0.jar has been previously copied to /tmp/spark-2a7d7f5d-e051-45d0-86ba-f2db8e507a91/userFiles-976854ae-e41a-4c1e-b4d4-09ce6e839fc3/io.delta_delta-core_2.12-2.1.0.jar\n",
      "23/06/08 18:41:48 INFO Executor: Fetching file:///home/hadoop/.ivy2/jars/org.codehaus.jackson_jackson-core-asl-1.9.13.jar with timestamp 1686249705953\n",
      "23/06/08 18:41:48 INFO Utils: /home/hadoop/.ivy2/jars/org.codehaus.jackson_jackson-core-asl-1.9.13.jar has been previously copied to /tmp/spark-2a7d7f5d-e051-45d0-86ba-f2db8e507a91/userFiles-976854ae-e41a-4c1e-b4d4-09ce6e839fc3/org.codehaus.jackson_jackson-core-asl-1.9.13.jar\n",
      "23/06/08 18:41:48 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41417.\n",
      "23/06/08 18:41:48 INFO NettyBlockTransferService: Server created on heracross.fib.upc.es:41417\n",
      "23/06/08 18:41:48 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "23/06/08 18:41:48 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, heracross.fib.upc.es, 41417, None)\n",
      "23/06/08 18:41:48 INFO BlockManagerMasterEndpoint: Registering block manager heracross.fib.upc.es:41417 with 1007.8 MiB RAM, BlockManagerId(driver, heracross.fib.upc.es, 41417, None)\n",
      "23/06/08 18:41:48 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, heracross.fib.upc.es, 41417, None)\n",
      "23/06/08 18:41:48 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, heracross.fib.upc.es, 41417, None)\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DeltaLakeToNeo4j\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\\\n",
    "    .config(\"spark.jars\", \"/neo4j-connector/neo4j-connector-apache-spark_2.12_3.0-4.0.0.jar\") \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.1.0\") \\\n",
    "    .config(\"spark.neo4j.bolt.url\", \"neo4j://10.4.41.56:7687\") \\\n",
    "    .config(\"spark.neo4j.bolt.user\", \"neo4j\") \\\n",
    "    .config(\"spark.neo4j.bolt.password\", \"foodie\")\\\n",
    "    .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66688f71-9cd5-465a-a11f-013f531a83eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = GraphDatabase.driver(\"neo4j://10.4.41.56:7687\", auth=(\"neo4j\", \"foodie\"))\n",
    "session = driver.session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c073635-946f-4caf-8c32-a1a479c6f752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Node element_id='573565' labels=frozenset({'Dishes'}) properties={'name': 'Pork belly buns', 'description': '28-day aged 300g USDA Certified Prime Ribeye, rosemary-thyme garlic butter, with choice of two sides.', 'id': 8, 'restaurantId': 1}>\n"
     ]
    }
   ],
   "source": [
    "# Create a session to run queries\n",
    "with driver.session() as session:\n",
    "    # Run the query\n",
    "    query = \"\"\"\n",
    "        MATCH (dish:Dishes {id: 8})\n",
    "        RETURN dish\n",
    "    \"\"\"\n",
    "    result = session.run(query)\n",
    "    \n",
    "    # Print the result\n",
    "    for record in result:\n",
    "        dish = record[\"dish\"]\n",
    "        print(dish)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "986e662d-98e9-460d-8ee4-3de054e77949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neo4j\n",
      "system\n"
     ]
    }
   ],
   "source": [
    "# Open a session\n",
    "with driver.session() as session:\n",
    "    # Execute a query to retrieve the list of databases\n",
    "    result = session.run(\"SHOW DATABASES\")\n",
    "\n",
    "    # Print the names of the databases\n",
    "    for record in result:\n",
    "        database_name = record[\"name\"]\n",
    "        print(database_name)\n",
    "\n",
    "# Close the driver\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c94fc599-338a-483a-b5ad-f358a3eeb66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # UNCOMMENT IF YOU WANT TO RELOAD ALL NODES\n",
    "# batch_size = 1000\n",
    "\n",
    "# # Delete nodes in batches\n",
    "# with driver.session() as session:\n",
    "#     while True:\n",
    "#         # Start a new transaction for each batch\n",
    "#         with session.begin_transaction() as tx:\n",
    "#             # Match and delete nodes in the current batch\n",
    "#             result = tx.run(\"\"\"\n",
    "#                 MATCH (n)\n",
    "#                 WITH n LIMIT $batchSize\n",
    "#                 DETACH DELETE n\n",
    "#                 RETURN count(n)\n",
    "#                 \"\"\", batchSize=batch_size)\n",
    "#             deleted_count = result.single()[0]\n",
    "\n",
    "#             # If no nodes were deleted, we have deleted all nodes\n",
    "#             if deleted_count == 0:\n",
    "#                 break\n",
    "\n",
    "# # Close the Neo4j driver\n",
    "# driver.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4e2796ba-1936-42a1-8504-80983fb18745",
   "metadata": {},
   "source": [
    "### Creating the nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7ccd9c8-a5f6-4a89-8f01-baeeca4488e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Delta Lake table into a DataFrame\n",
    "df_michelin = spark.read.format(\"delta\").load(\"hdfs://localhost:9000/user/hadoop/delta/warehouse/michelin\")\n",
    "\n",
    "# Create a temporary view for the DataFrame\n",
    "df_michelin.createOrReplaceTempView(\"michelin\")\n",
    "\n",
    "with driver.session() as session:\n",
    "    # Define the Cypher query to create nodes\n",
    "    for row in df_michelin.collect():\n",
    "        # Create a node for each row\n",
    "        session.run(\n",
    "            \"MERGE (m:Michelin {ra_key: $ra_key})\"\n",
    "            \"SET m.number_of_stars = $number_of_stars, m.year = $year\",\n",
    "            ra_key=row.ra_key,\n",
    "            number_of_stars=row.number_of_stars,\n",
    "            year=row.year\n",
    "        )\n",
    "\n",
    "driver.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "14931d73-df5f-48a5-918a-e14083158d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Delta Lake table into a DataFrame\n",
    "df_restaurant = spark.read.format(\"delta\").load(\"hdfs://localhost:9000/user/hadoop/delta/warehouse/restaurants\")\n",
    "\n",
    "# Create a temporary view for the DataFrame\n",
    "df_restaurant.createOrReplaceTempView(\"restaurant\")\n",
    "\n",
    "with driver.session() as session:\n",
    "    # Iterate over the rows of the DataFrame\n",
    "    for row in df_restaurant.collect():\n",
    "        # Create a node for each row\n",
    "        session.run(\n",
    "            \"MERGE (r:Restaurant {key: $key})\"\n",
    "            \"SET r.restaurant_name = $restaurant_name, r.address = $address, r.latitude = $latitude, \\\n",
    "            r.longitude = $longitude, r.website = $website, r.email = $email, r.telephone = $telephone, \\\n",
    "            r.ta_key = $ta_key, r.go_key = $go_key, r.closed = $closed\",\n",
    "            key=row.key,\n",
    "            restaurant_name=row.restaurant_name,\n",
    "            address=row.address,\n",
    "            latitude=row.latitude,\n",
    "            longitude=row.longitude,\n",
    "            website=row.website,\n",
    "            email=row.email,\n",
    "            telephone=row.telephone,\n",
    "            ta_key=row.ta_key,\n",
    "            go_key=row.go_key,\n",
    "            closed=row.closed\n",
    "        )\n",
    "\n",
    "driver.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9377c1ec-0fdf-489d-8c0e-164a6ac5ae59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Delta Lake table into a DataFrame\n",
    "df_images = spark.read.format(\"delta\").load(\"hdfs://localhost:9000/user/hadoop/delta/warehouse/images\")\n",
    "\n",
    "# Create a temporary view for the DataFrame\n",
    "df_images.createOrReplaceTempView(\"images\")\n",
    "\n",
    "with driver.session() as session:\n",
    "    for row in df_images.collect():\n",
    "        session.run(\n",
    "            \"MERGE (r:Images {id: $id}) \"\n",
    "            \"SET r.file_url = $file_url, r.dishId = $dishId, r.uploaderId = $uploaderId, r.uploadDt = $uploadDt\",\n",
    "            id=row.id,\n",
    "            file_url=row.file_url,\n",
    "            dishId=row.dishId,\n",
    "            uploaderId=row.uploaderId,\n",
    "            uploadDt=row.uploadDt\n",
    "        )\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0422354b-21ea-40a6-8489-ca868545e9e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Load the Delta Lake table into a DataFrame\n",
    "df_images_ex = spark.read.format(\"delta\").load(\"hdfs://localhost:9000/user/hadoop/delta/warehouse/images_external\")\n",
    "\n",
    "# Create a temporary view for the DataFrame\n",
    "df_images_ex.createOrReplaceTempView(\"images\")\n",
    "\n",
    "with driver.session() as session:\n",
    "    # Iterate over the rows of the DataFrame\n",
    "    for row in df_images_ex.collect():\n",
    "        # Create a node for each row\n",
    "        session.run(\n",
    "            \"MERGE (i:Images_ex {image_id: $image_id})\"\n",
    "            \"SET i.ra_key = $ra_key, i.image_file = $image_file, i.source = $source\",\n",
    "            image_id=row.image_id,\n",
    "            ra_key=row.ra_key,\n",
    "            image_file=row.image_file,\n",
    "            source=row.source\n",
    "\n",
    "        )\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4892db39-40e7-4439-98ba-48dc7f465b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Load the Delta Lake table into a DataFrame\n",
    "df_cuisines = spark.read.format(\"delta\").load(\"hdfs://localhost:9000/user/hadoop/delta/warehouse/cuisines\")\n",
    "\n",
    "# Create a temporary view for the DataFrame\n",
    "df_cuisines.createOrReplaceTempView(\"cuisines\")\n",
    "\n",
    "# Define the Cypher query to create nodes\n",
    "with driver.session() as session:\n",
    "    # Iterate over the rows of the DataFrame\n",
    "    for row in df_cuisines.collect():\n",
    "        # Create a node for each row\n",
    "        session.run(\n",
    "            \"MERGE (c:Cuisines {cuisine_key: $cuisine_key})\"\n",
    "            \"SET c.restaurant_key = $restaurant_key, c.cuisine = $cuisine\",\n",
    "            cuisine_key=row.cuisine_key,\n",
    "            restaurant_key=row.restaurant_key,\n",
    "            cuisine=row.cuisine\n",
    "        )\n",
    "driver.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3136079-9f47-4fc1-84c0-896da5c37424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Delta Lake table into a DataFrame\n",
    "df_long_review = spark.read.format(\"delta\").load(\"hdfs://localhost:9000/user/hadoop/delta/warehouse/long_review\")\n",
    "\n",
    "# Create a temporary view for the DataFrame\n",
    "df_long_review.createOrReplaceTempView(\"review_long\")\n",
    "\n",
    "with driver.session() as session:\n",
    "    # Iterate over the rows of the DataFrame\n",
    "    for row in df_long_review.collect():\n",
    "        # Create a node for each row\n",
    "        session.run(\n",
    "            \"MERGE (r:Review {review_key: $review_key})\"\n",
    "            \"SET r.time = $time, r.rating = $rating, r.ra_key = $ra_key, r.sample = $sample, r.text = $text, r.source = $source\",\n",
    "            review_key=row.review_key,\n",
    "            time=row.time,\n",
    "            rating=row.rating,\n",
    "            ra_key=row.ra_key,\n",
    "            sample=row.sample,\n",
    "            text=row.text,\n",
    "            source=row.source\n",
    "        )\n",
    "\n",
    "# Close the Neo4j driver\n",
    "driver.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfb26f6-65f1-4437-825d-9231ee44a7e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Load the Delta Lake table into a DataFrame\n",
    "df_review = spark.read.format(\"delta\").load(\"hdfs://localhost:9000/user/hadoop/delta/warehouse/reviews\")\n",
    "\n",
    "# Create a temporary view for the DataFrame\n",
    "df_review.createOrReplaceTempView(\"reviews\")\n",
    "\n",
    "with driver.session() as session:\n",
    "    # Iterate over the rows of the DataFrame\n",
    "    for row in df_review.collect():\n",
    "        # Create a node for each row\n",
    "        session.run(\n",
    "            \"MERGE (r:Review {id: $id})\"\n",
    "            \"SET r.userId = $userId, r.dishId = $dishId, r.rating = $rating, r.timestamp = $timestamp, r.text = $text\",\n",
    "            id=row.id,\n",
    "            userId=row.userId,\n",
    "            dishId=row.dishId,\n",
    "            rating=row.rating,\n",
    "            timestamp=row.timestamp,\n",
    "            text=row.text\n",
    "        )\n",
    "\n",
    "# Close the Neo4j driver\n",
    "driver.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58db1d0f-9255-443d-989b-d92d82c42552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Delta Lake table into a DataFrame\n",
    "df_user = spark.read.format(\"delta\").load(\"hdfs://localhost:9000/user/hadoop/delta/warehouse/users\")\n",
    "\n",
    "# Create a temporary view for the DataFrame\n",
    "df_user.createOrReplaceTempView(\"users\")\n",
    "\n",
    "# Define the Cypher query to create nodes\n",
    "with driver.session() as session:\n",
    "    # Iterate over the rows of the DataFrame\n",
    "    for row in df_user.collect():\n",
    "        # Create a node for each row\n",
    "        session.run(\n",
    "            \"MERGE (u:User {id: $id})\"\n",
    "            \"SET u.sex = $sex, u.createDate = $createDate, u.nationality = $nationality, u.occupation = $occupation\",\n",
    "            id=row.id,\n",
    "            sex=row.sex,\n",
    "            createDate=row.createDate,\n",
    "            nationality=row.nationality,\n",
    "            occupation=row.occupation\n",
    "        )\n",
    "driver.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1d114f2-7fa8-459c-bf0e-8903942c7207",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Load the Delta Lake table into a DataFrame\n",
    "df_weekday = spark.read.format(\"delta\").load(\"hdfs://localhost:9000/user/hadoop/delta/warehouse/weekday\")\n",
    "\n",
    "# Create a temporary view for the DataFrame\n",
    "df_weekday.createOrReplaceTempView(\"weekday\")\n",
    "\n",
    "with driver.session() as session:\n",
    "    # Iterate over the rows of the DataFrame\n",
    "    for row in df_weekday.collect():\n",
    "        # Create a node for each row\n",
    "        session.run(\n",
    "            \"MERGE (w:Weekday {day_key: $day_key})\"\n",
    "            \"SET w.weekday = $weekday\",\n",
    "            day_key=row.day_key,\n",
    "            weekday=row.weekday\n",
    "        )\n",
    "\n",
    "# Close the Neo4j driver\n",
    "driver.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22c511b9-9619-45f8-8f5c-0c8155a31bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Load the Delta Lake table into a DataFrame\n",
    "df_dishes = spark.read.format(\"delta\").load(\"hdfs://localhost:9000/user/hadoop/delta/warehouse/dishes\")\n",
    "\n",
    "# Create a temporary view for the DataFrame\n",
    "df_dishes.createOrReplaceTempView(\"dishes\")\n",
    "\n",
    "with driver.session() as session:\n",
    "    # Iterate over the rows of the DataFrame\n",
    "    for row in df_dishes.collect():\n",
    "        # Create a node for each row\n",
    "        session.run(\n",
    "            \"MERGE (d:Dishes {id: $id})\"\n",
    "            \"SET d.name = $name, d.description = $description, d.restaurantId = $restaurantId\",\n",
    "            id=row.id,\n",
    "            name=row.name,\n",
    "            description=row.description,\n",
    "            restaurantId=row.restaurantId\n",
    "        )\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f8772d8-67d9-4a79-a572-13eeb013c971",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Load the Delta Lake table into a DataFrame\n",
    "df_ingredients = spark.read.format(\"delta\").load(\"hdfs://localhost:9000/user/hadoop/delta/warehouse/ingredients\")\n",
    "\n",
    "# Create a temporary view for the DataFrame\n",
    "df_ingredients.createOrReplaceTempView(\"ingredients\")\n",
    "\n",
    "with driver.session() as session:\n",
    "    # Iterate over the rows of the DataFrame\n",
    "    for row in df_ingredients.collect():\n",
    "        # Create a node for each row\n",
    "        session.run(\n",
    "            \"MERGE (d:Ingredients{id: $id})\"\n",
    "            \"SET d.ingredient = $ingredient\",\n",
    "             id=row.id,\n",
    "             ingredient=row.ingredient\n",
    "           \n",
    "        )\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ef434e02-6822-41d4-9cf1-e1f5d378b3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Delta Lake table into a DataFrame\n",
    "df_tags = spark.read.format(\"delta\").load(\"hdfs://localhost:9000/user/hadoop/delta/warehouse/tags\")\n",
    "\n",
    "# Create a temporary view for the DataFrame\n",
    "df_tags.createOrReplaceTempView(\"tags\")\n",
    "\n",
    "with driver.session() as session:\n",
    "    # Iterate over the rows of the DataFrame\n",
    "    for row in df_tags.collect():\n",
    "        # Create a node for each row\n",
    "        session.run(\n",
    "            \"MERGE (d:Tags {id: $id})\"\n",
    "            \"SET d.tag = $tag\",\n",
    "            id=row.id,\n",
    "            tag=row.tag\n",
    "        )\n",
    "\n",
    "# Close the Neo4j driver\n",
    "driver.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a53746e-4a89-496e-b759-f8ba2a12e1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Load the Delta Lake table into a DataFrame\n",
    "df_opening_hours = spark.read.format(\"delta\").load(\"hdfs://localhost:9000/user/hadoop/delta/warehouse/opening_hours\")\n",
    "\n",
    "# Create a temporary view for the DataFrame\n",
    "df_opening_hours.createOrReplaceTempView(\"opening_hours\")\n",
    "\n",
    "# Define the Cypher query to create nodes\n",
    "with driver.session() as session:\n",
    "    # Iterate over the rows of the DataFrame\n",
    "    for row in df_opening_hours.collect():\n",
    "        # Create a node for each row\n",
    "        session.run(\n",
    "            \"MERGE (o:Opening_hours {ra_key: $ra_key, day_key: $day_key})\"\n",
    "            \"SET o.open_time = $open_time, o.close_time = $close_time\",\n",
    "            ra_key=row.ra_key,\n",
    "            day_key=row.day_key,\n",
    "            open_time=row.open_time,\n",
    "            close_time=row.close_time\n",
    "            \n",
    "\n",
    "        )\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f4b560e-eb0e-47d6-a30b-388d2867cc14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Load the Delta Lake table into a DataFrame\n",
    "df_interestedCuisines = spark.read.format(\"delta\").load(\"hdfs://localhost:9000/user/hadoop/delta/warehouse/interestedCuisines\")\n",
    "\n",
    "# Create a temporary view for the DataFrame\n",
    "df_interestedCuisines.createOrReplaceTempView(\"interestedCuisines\")\n",
    "\n",
    "# Define the Cypher query to create nodes\n",
    "with driver.session() as session:\n",
    "    # Iterate over the rows of the DataFrame\n",
    "    for row in df_interestedCuisines.collect():\n",
    "        # Create a node for each row\n",
    "           session.run(\n",
    "            \"MERGE (c:interestedCuisines {id: $id})\"\n",
    "            \"SET c.interestedCuisine = $interestedCuisine\",\n",
    "            id=row.id,\n",
    "            interestedCuisine=row.interestedCuisine\n",
    "        )\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cf3d17fa-cfd0-4cb8-bd24-665c4b179345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Delta Lake table into a DataFrame\n",
    "df_meals = spark.read.format(\"delta\").load(\"hdfs://localhost:9000/user/hadoop/delta/warehouse/meals\")\n",
    "\n",
    "# Create a temporary view for the DataFrame\n",
    "df_meals.createOrReplaceTempView(\"meals\")\n",
    "\n",
    "# Define the Cypher query to create nodes\n",
    "with driver.session() as session:\n",
    "    # Iterate over the rows of the DataFrame\n",
    "    for row in df_meals.collect():\n",
    "        # Create a node for each row\n",
    "           session.run(\n",
    "            \"MERGE (c:Meals {meal_key: $meal_key})\"\n",
    "            \"SET c.restaurant_key=$restaurant_key, c.meals = $meals\",\n",
    "            meal_key=row.meal_key,\n",
    "            restaurant_key=row.restaurant_key,\n",
    "            meals=row.meals\n",
    "        )\n",
    "driver.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "41befebc-1675-4ce5-80d8-c50f8c8d007d",
   "metadata": {},
   "source": [
    "### Creating relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f7b890b9-96e3-4458-8356-bcf067269706",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Read DataFrames\n",
    "user_df = spark.read.format(\"delta\").load(\"hdfs://localhost:9000/user/hadoop/delta/warehouse/users\")\n",
    "user_cuisine_df = spark.read.format(\"delta\").load(\"hdfs://localhost:9000/user/hadoop/delta/warehouse/user_cuisines\")\n",
    "cuisine_df = spark.read.format(\"delta\").load(\"hdfs://localhost:9000/user/hadoop/delta/warehouse/interestedCuisines\")\n",
    "\n",
    "# Register the DataFrames as temporary views\n",
    "user_df.createOrReplaceTempView(\"users\")\n",
    "user_cuisine_df.createOrReplaceTempView(\"user_cuisines\")\n",
    "cuisine_df.createOrReplaceTempView(\"interestedCuisines\")\n",
    "\n",
    "# Define the Cypher query\n",
    "query = \"\"\"\n",
    "MERGE (u:User {id: $user_id})\n",
    "MERGE (c:interestedCuisines {id: $cuisine_id})\n",
    "CREATE (u)-[:INTERESTED_IN]->(c)\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query for each row in the intermediate table\n",
    "with driver.session() as session:\n",
    "    for row in user_cuisine_df.collect():\n",
    "        user_id = row['userId']\n",
    "        cuisine_id = row['interestedCuisineId']\n",
    "        session.run(query, user_id=user_id, cuisine_id=cuisine_id)\n",
    "\n",
    "# Close the Neo4j driver\n",
    "driver.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aac82f53-e23d-4ded-9b91-fa796c742882",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Read Spark DataFrames\n",
    "user_df = spark.read.format(\"delta\").load(\"hdfs://localhost:9000/user/hadoop/delta/warehouse/users\")\n",
    "image_df = spark.read.format(\"delta\").load(\"hdfs://localhost:9000/user/hadoop/delta/warehouse/images\")\n",
    "\n",
    "# Register the DataFrames as temporary views\n",
    "user_df.createOrReplaceTempView(\"users\")\n",
    "image_df.createOrReplaceTempView(\"images\")\n",
    "\n",
    "# Define the Cypher query\n",
    "query = \"\"\"\n",
    "    MERGE (u:User {id: $uploader_id})\n",
    "    MERGE (i:Images {id: $imageId})\n",
    "    CREATE (u)-[:UPLOADED]->(i)\n",
    "    \"\"\"\n",
    "\n",
    "# Execute the query for each row in the DataFrames\n",
    "with driver.session() as session:\n",
    "    for image_row in image_df.collect():\n",
    "        uploader_id = image_row['uploaderId']\n",
    "        imageId = image_row['id']\n",
    "        session.run(query, uploader_id=uploader_id, imageId=imageId)\n",
    "\n",
    "# Close the Neo4j driver\n",
    "driver.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "37ffe1ba-c93f-4d53-aa6b-1e6f07d455fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Spark DataFrames\n",
    "user_df = spark.read.format(\"delta\").load(\"hdfs://localhost:9000/user/hadoop/delta/warehouse/users\")\n",
    "locations_df = spark.read.format(\"delta\").load(\"hdfs://localhost:9000/user/hadoop/delta/warehouse/locations\")\n",
    "\n",
    "# Register the DataFrames as temporary views\n",
    "user_df.createOrReplaceTempView(\"users\")\n",
    "locations_df.createOrReplaceTempView(\"locations\")\n",
    "\n",
    "# Define the Cypher query\n",
    "query = \"\"\"\n",
    "    MERGE (u:User {id: $userId})\n",
    "    MERGE (i:Locations {id: $id})\n",
    "    CREATE (u)-[:IS_LOCATED]->(i)\n",
    "    \"\"\"\n",
    "\n",
    "# Execute the query for each row in the DataFrames\n",
    "with driver.session() as session:\n",
    "    for locations_row in locations_df.collect():\n",
    "        userId = locations_row['userId']\n",
    "        id = locations_row['id']\n",
    "        session.run(query, id=id, userId=userId)\n",
    "\n",
    "# Close the Neo4j driver\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b130b981-6025-4e45-b933-66e46db760cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Spark DataFrames\n",
    "restaurants_df = spark.read.format(\"delta\").load(\"hdfs://localhost:9000/user/hadoop/delta/warehouse/restaurants\")\n",
    "image_df = spark.read.format(\"delta\").load(\"hdfs://localhost:9000/user/hadoop/delta/warehouse/images\")\n",
    "\n",
    "# Register the DataFrames as temporary views\n",
    "restaurants_df.createOrReplaceTempView(\"restaurants\")\n",
    "image_df.createOrReplaceTempView(\"images\")\n",
    "\n",
    "# Define the Cypher query\n",
    "query = \"\"\"\n",
    "    MATCH (r:Restaurant {id: $key})\n",
    "    MERGE (i:Images {id: $imageId})\n",
    "    CREATE (r)-[:HAS_IMAGE]->(i)\n",
    "    \"\"\"\n",
    "\n",
    "# Execute the query for each row in the DataFrames\n",
    "with driver.session() as session:\n",
    "    for image_row in image_df.collect():\n",
    "        key = image_row['restaurantId']\n",
    "        imageId = image_row['id']\n",
    "        session.run(query, key=key, imageId=imageId)\n",
    "\n",
    "# Close the Neo4j driver\n",
    "driver.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4c38aac0-2052-4989-889a-ad0141769668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Spark DataFrames\n",
    "restaurants_df = spark.read.format(\"delta\").load(\"hdfs://localhost:9000/user/hadoop/delta/warehouse/restaurants\")\n",
    "images_external_df = spark.read.format(\"delta\").load(\"hdfs://localhost:9000/user/hadoop/delta/warehouse/images_external\")\n",
    "\n",
    "# Register the DataFrames as temporary views\n",
    "restaurants_df.createOrReplaceTempView(\"restaurants\")\n",
    "images_external_df.createOrReplaceTempView(\"images_external\")\n",
    "\n",
    "# Define the Cypher query\n",
    "query = \"\"\"\n",
    "    MATCH (r:Restaurant {key: $ra_key})\n",
    "    MERGE (i:Images_ex {image_id: $image_id})\n",
    "    CREATE (i)-[:EX_BELONGS_TO]->(r)\n",
    "    \"\"\"\n",
    "\n",
    "# Execute the query for each row in the DataFrames\n",
    "with driver.session() as session:\n",
    "    for images_external_row in images_external_df.collect():\n",
    "        ra_key = images_external_row['ra_key']\n",
    "        image_id = images_external_row['image_id']\n",
    "        session.run(query, ra_key=ra_key, image_id=image_id)\n",
    "\n",
    "# Close the Neo4j driver\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "27fd3f8d-c567-4040-8f4c-5b5d873ec1d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Read DataFrames\n",
    "image_tags_df = spark.read.format(\"delta\").load(\"hdfs://localhost:9000/user/hadoop/delta/warehouse/image_tags\")\n",
    "tag_df = spark.read.format(\"delta\").load(\"hdfs://localhost:9000/user/hadoop/delta/warehouse/tags\")\n",
    "images_df = spark.read.format(\"delta\").load(\"hdfs://localhost:9000/user/hadoop/delta/warehouse/images\")\n",
    "\n",
    "# Register the DataFrames as temporary views\n",
    "image_tags_df.createOrReplaceTempView(\"image_tags\")\n",
    "tag_df.createOrReplaceTempView(\"tags\")\n",
    "images_df.createOrReplaceTempView(\"images\")\n",
    "\n",
    "# Define the Cypher query\n",
    "query = \"\"\"\n",
    "MERGE (u:Images {id: $imageId})\n",
    "MERGE (c:Tags {id: $tagId})\n",
    "CREATE (u)-[:HAS_TAG]->(c)\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query for each row in the intermediate table\n",
    "with driver.session() as session:\n",
    "    for row in image_tags_df.collect():\n",
    "        imageId = row['imageId']\n",
    "        tagId = row['tagId']\n",
    "        session.run(query, imageId=imageId, tagId=tagId)\n",
    "\n",
    "# Close the Neo4j driver\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "505d7e55-05ba-448d-8542-49b024cc4809",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12534/2820214140.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0muserId\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreviews_row\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'userId'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreviews_row\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserId\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muserId\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Close the Neo4j driver\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/neo4j/_sync/work/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, query, parameters, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0mbookmarks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_bookmarks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m         self._auto_result._run(\n\u001b[0m\u001b[1;32m    312\u001b[0m             \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatabase\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpersonated_user\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_access_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/neo4j/_sync/work/result.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, query, parameters, db, imp_user, access_mode, bookmarks, notifications_min_severity, notifications_disabled_categories)\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_attach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_pull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/neo4j/_sync/work/result.py\u001b[0m in \u001b[0;36m_attach\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exhausted\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_attached\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/neo4j/_sync/io/_common.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                     \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mNeo4jError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mServiceUnavailable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSessionExpired\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m                     \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miscoroutinefunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__on_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/neo4j/_sync/io/_bolt.py\u001b[0m in \u001b[0;36mfetch_message\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    803\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    804\u001b[0m         \u001b[0;31m# Receive exactly one message\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 805\u001b[0;31m         tag, fields = self.inbox.pop(\n\u001b[0m\u001b[1;32m    806\u001b[0m             \u001b[0mhydration_hooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhydration_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m         )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/neo4j/_sync/io/_common.py\u001b[0m in \u001b[0;36mpop\u001b[0;34m(self, hydration_hooks)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhydration_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer_one_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unpacker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_structure_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/neo4j/_sync/io/_common.py\u001b[0m in \u001b[0;36m_buffer_one_chunk\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0mchunk_size\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                     \u001b[0;31m# Determine the chunk size and skip noop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m                     \u001b[0mreceive_into_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m                     \u001b[0mchunk_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop_u16\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mchunk_size\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/neo4j/_sync/io/_common.py\u001b[0m in \u001b[0;36mreceive_into_buffer\u001b[0;34m(sock, buffer, n_bytes)\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mview\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mused\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mused\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mused\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/neo4j/_async_compat/network/_bolt_socket.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_io\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_socket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msendall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/neo4j/_async_compat/network/_bolt_socket.py\u001b[0m in \u001b[0;36m_wait_for_io\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_wait_for_io\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_deadline\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m         \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_socket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[0mdeadline_timeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_deadline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Read the user and image data into Spark DataFrames\n",
    "user_df = spark.read.format(\"delta\").load(\"hdfs://localhost:9000/user/hadoop/delta/warehouse/users\")\n",
    "reviews_df = spark.read.format(\"delta\").load(\"hdfs://localhost:9000/user/hadoop/delta/warehouse/reviews\")\n",
    "\n",
    "# Register the DataFrames as temporary views\n",
    "user_df.createOrReplaceTempView(\"users\")\n",
    "reviews_df.createOrReplaceTempView(\"images\")\n",
    "\n",
    "# Define the Cypher query\n",
    "query = \"\"\"\n",
    "    MERGE (u:User {id: $userId})\n",
    "    MERGE (i:Review {id: $id})\n",
    "    CREATE (u)-[:WROTE]->(i)\n",
    "    \"\"\"\n",
    "\n",
    "# Execute the query for each row in the DataFrames\n",
    "with driver.session() as session:\n",
    "    for reviews_row in reviews_df.collect():\n",
    "        userId = reviews_row['userId']\n",
    "        id = reviews_row['id']\n",
    "        session.run(query, userId=userId, id=id)\n",
    "\n",
    "# Close the Neo4j driver\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70f9054b-4da6-4566-8303-11722c286ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Read Spark DataFrames\n",
    "restaurants_df = spark.read.format(\"delta\").load(\"hdfs://localhost:9000/user/hadoop/delta/warehouse/restaurants\")\n",
    "dish_df = spark.read.format(\"delta\").load(\"hdfs://localhost:9000/user/hadoop/delta/warehouse/dishes\")\n",
    "\n",
    "# Register the DataFrames as temporary views\n",
    "restaurants_df.createOrReplaceTempView(\"restaurants\")\n",
    "dish_df.createOrReplaceTempView(\"dishes\")\n",
    "\n",
    "# Define the Cypher query\n",
    "query = \"\"\"\n",
    "    MERGE (r:Dishes {id: $id})\n",
    "    MERGE (i:Restaurant {key: $restaurantId})\n",
    "    CREATE (r)-[:ON_MENU]->(i)\n",
    "    \"\"\"\n",
    "\n",
    "# Execute the query for each row in the DataFrames\n",
    "with driver.session() as session:\n",
    "    for dish_row in dish_df.collect():\n",
    "        restaurantId = dish_row['restaurantId']\n",
    "        id = dish_row['id']\n",
    "        session.run(query, restaurantId=restaurantId, id=id)\n",
    "\n",
    "# Close the Neo4j driver\n",
    "driver.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "686db318-934a-4964-bb68-0122e65141c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Spark DataFrames\n",
    "ingredients_df = spark.read.format(\"delta\").load(\"hdfs://localhost:9000/user/hadoop/delta/warehouse/ingredients\")\n",
    "dish_df = spark.read.format(\"delta\").load(\"hdfs://localhost:9000/user/hadoop/delta/warehouse/dishes\")\n",
    "ingredients_in_dish_df = spark.read.format(\"delta\").load(\"hdfs://localhost:9000/user/hadoop/delta/warehouse/ingredients_in_dish\")\n",
    "\n",
    "# Register the DataFrames as temporary views\n",
    "ingredients_df.createOrReplaceTempView(\"restaurants\")\n",
    "dish_df.createOrReplaceTempView(\"dishes\")\n",
    "ingredients_in_dish_df.createOrReplaceTempView(\"ingredients_in_dish\")\n",
    "\n",
    "# Define the Cypher query\n",
    "query = \"\"\"\n",
    "MERGE (u:Dishes {id: $dishId})\n",
    "MERGE (c:Ingredients {id: $ingredientId})\n",
    "CREATE (u)-[:MADE_OF]->(c)\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query for each row in the intermediate table\n",
    "with driver.session() as session:\n",
    "    for row in ingredients_in_dish_df.collect():\n",
    "        dishId = row['dishId']\n",
    "        ingredientId = row['ingredientId']\n",
    "        session.run(query, dishId=dishId, ingredientId=ingredientId)\n",
    "\n",
    "# Close the Neo4j driver\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c628b6f4-adaf-4a48-82a8-d91ab0ad595b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Read Spark DataFrames\n",
    "restaurants_df = spark.read.format(\"delta\").load(\"hdfs://localhost:9000/user/hadoop/delta/warehouse/restaurants\")\n",
    "cuisines_df = spark.read.format(\"delta\").load(\"hdfs://localhost:9000/user/hadoop/delta/warehouse/cuisines\")\n",
    "\n",
    "# Register the DataFrames as temporary views\n",
    "restaurants_df.createOrReplaceTempView(\"restaurants\")\n",
    "cuisines_df.createOrReplaceTempView(\"cuisines\")\n",
    "\n",
    "# Define the Cypher query\n",
    "query = \"\"\"\n",
    "    MERGE (r:Restaurant {key: $restaurant_key})\n",
    "    MERGE (c:Cuisines {id: $cuisine_key})\n",
    "    CREATE (r)-[:HAS_CUISINES]->(c)\n",
    "    \"\"\"\n",
    "\n",
    "# Execute the query for each row in the DataFrames\n",
    "with driver.session() as session:\n",
    "    for cuisines_row in cuisines_df.collect():\n",
    "        restaurant_key = cuisines_row['restaurant_key']\n",
    "        cuisine_key = cuisines_row['cuisine_key']\n",
    "        session.run(query, restaurant_key=restaurant_key, cuisine_key=cuisine_key)\n",
    "\n",
    "# Close the Neo4j driver\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "508cd616-cd37-4aa0-83aa-0386ea23dec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Spark DataFrames\n",
    "restaurants_df = spark.read.format(\"delta\").load(\"hdfs://localhost:9000/user/hadoop/delta/warehouse/restaurants\")\n",
    "meals_df = spark.read.format(\"delta\").load(\"hdfs://localhost:9000/user/hadoop/delta/warehouse/meals\")\n",
    "\n",
    "# Register the DataFrames as temporary views\n",
    "restaurants_df.createOrReplaceTempView(\"restaurants\")\n",
    "meals_df.createOrReplaceTempView(\"meals\")\n",
    "\n",
    "# Define the Cypher query\n",
    "query = \"\"\"\n",
    "    MERGE (r:Restaurant {key: $restaurant_key})\n",
    "    WITH r\n",
    "    MATCH (c:Meals {meal_key: $meal_key})\n",
    "    CREATE (r)-[:OFFERS_MEALS]->(c)\n",
    "    \"\"\"\n",
    "\n",
    "# Execute the query for each row in the DataFrames\n",
    "with driver.session() as session:\n",
    "    for meals_row in meals_df.collect():\n",
    "        restaurant_key = meals_row['restaurant_key']\n",
    "        meal_key = meals_row['meal_key']\n",
    "        session.run(query, restaurant_key=restaurant_key, meal_key=meal_key)\n",
    "\n",
    "# Close the Neo4j driver\n",
    "driver.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "488ce030-388d-4196-92c5-294027fcc267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Spark DataFrames\n",
    "restaurants_df = spark.read.format(\"delta\").load(\"hdfs://localhost:9000/user/hadoop/delta/warehouse/restaurants\")\n",
    "opening_hours_df = spark.read.format(\"delta\").load(\"hdfs://localhost:9000/user/hadoop/delta/warehouse/opening_hours\")\n",
    "weekday_df = spark.read.format(\"delta\").load(\"hdfs://localhost:9000/user/hadoop/delta/warehouse/weekday\")\n",
    "\n",
    "# Register the DataFrames as temporary views\n",
    "restaurants_df.createOrReplaceTempView(\"restaurants\")\n",
    "opening_hours_df.createOrReplaceTempView(\"opening_hours\")\n",
    "weekday_df.createOrReplaceTempView(\"weekday\")\n",
    "\n",
    "# Define the Cypher query\n",
    "query = \"\"\"\n",
    "     MATCH (r:Restaurant {key: $ra_key})\n",
    "    MATCH (w:Weekday {day_key: $day_key})\n",
    "    MERGE (r)-[o:OPENING_HOURS]->(w)\n",
    "    SET o.open_time = $open_time, o.close_time = $close_time\n",
    "        \"\"\"\n",
    "\n",
    "# Execute the query for each row in the DataFrames\n",
    "with driver.session() as session:\n",
    "    for opening_hours_row in opening_hours_df.collect():\n",
    "        ra_key = opening_hours_row['ra_key']\n",
    "        day_key = opening_hours_row['day_key']\n",
    "        open_time=opening_hours_row['open_time']\n",
    "        close_time=opening_hours_row['close_time']\n",
    "        session.run(query, ra_key=ra_key, day_key=day_key, open_time=open_time,close_time=close_time)\n",
    "\n",
    "# Close the Neo4j driver\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c69908c4-b47d-4c24-a1d5-aa95b2c5e58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Spark DataFrames\n",
    "restaurants_df = spark.read.format(\"delta\").load(\"hdfs://localhost:9000/user/hadoop/delta/warehouse/restaurants\")\n",
    "michelin_df = spark.read.format(\"delta\").load(\"hdfs://localhost:9000/user/hadoop/delta/warehouse/michelin\")\n",
    "\n",
    "# Register the DataFrames as temporary views\n",
    "restaurants_df.createOrReplaceTempView(\"restaurants\")\n",
    "michelin_df.createOrReplaceTempView(\"michelin\")\n",
    "\n",
    "# Define the Cypher query\n",
    "query = \"\"\"\n",
    "    MATCH (r1:Restaurant {key: $ra_key1})\n",
    "    MATCH (r2:Restaurant {key: $ra_key2})\n",
    "    MERGE (r1)-[h:HAS_MICHELIN]->(r2)\n",
    "    SET h.number_of_stars = $number_of_stars, h.year = $year\n",
    "    \"\"\"\n",
    "\n",
    "# Execute the query for each row in the DataFrames\n",
    "with driver.session() as session:\n",
    "    for michelin_row in michelin_df.collect():\n",
    "        ra_key1 = michelin_row['ra_key']\n",
    "        ra_key2 = michelin_row['ra_key']\n",
    "        number_of_stars = michelin_row['number_of_stars']\n",
    "        year = michelin_row['year']\n",
    "        session.run(query, ra_key1=ra_key1, ra_key2=ra_key2, number_of_stars=number_of_stars, year=year)\n",
    "\n",
    "# Close the Neo4j driver\n",
    "driver.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3bd582-24e7-4cae-bdf1-abfd5fd9c63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Read Spark DataFrames\n",
    "user_df = spark.read.format(\"delta\").load(\"hdfs://localhost:9000/user/hadoop/delta/warehouse/users\")\n",
    "image_df = spark.read.format(\"delta\").load(\"hdfs://localhost:9000/user/hadoop/delta/warehouse/images\")\n",
    "swipes_df = spark.read.format(\"delta\").load(\"hdfs://localhost:9000/user/hadoop/delta/warehouse/swipes\")\n",
    "\n",
    "# Register the DataFrames as temporary views\n",
    "user_df.createOrReplaceTempView(\"users\")\n",
    "image_df.createOrReplaceTempView(\"image\")\n",
    "swipes_df.createOrReplaceTempView(\"swipes\")\n",
    "\n",
    "# Define the Cypher query\n",
    "query = \"\"\"\n",
    "    MERGE (u:User {id: $userId})\n",
    "    MERGE (i:Images {id: $imageId})\n",
    "    CREATE (u)-[s:SWIPED]->(i)\n",
    "    SET s.timestamp = $timestamp, s.swipeLeft = $swipeLeft\n",
    "    \"\"\"\n",
    "\n",
    "# Execute the query for each row in the DataFrames\n",
    "with driver.session() as session:\n",
    "    for swipes_row in swipes_df.collect():\n",
    "        userId = swipes_row['userId']\n",
    "        imageId = swipes_row['imageId']\n",
    "        timestamp = swipes_row['timestamp']\n",
    "        swipeLeft = swipes_row['swipeLeft']\n",
    "        session.run(query, userId=userId, imageId=imageId, timestamp=timestamp, swipeLeft=swipeLeft)\n",
    "\n",
    "# Close the Neo4j driver\n",
    "driver.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab10a236-a5f6-4710-abb3-9c23f72e6323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the SparkSession\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
